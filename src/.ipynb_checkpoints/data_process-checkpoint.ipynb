{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398ba2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 12:40:24.211551: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfmath\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Concatenate, Dense, Conv2D, LeakyReLU, ReLU, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from tensorflow.keras.regularizers import l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e02fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = None  # Set to None to use all events, or specify a number to limit\n",
    "test_size = 0.2  # 20% for test set\n",
    "val_size = 0.2   # 20% of training set for validation (so 16% of total)\n",
    "input_shape = -1\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../data/processed_data/', exist_ok=True)\n",
    "\n",
    "# Find all h5 files in the data directory\n",
    "data_files = glob.glob('../data/*.h5')\n",
    "\n",
    "# Separate background and signal files\n",
    "bkg_files = [f for f in data_files if os.path.basename(f).lower().startswith('background')]\n",
    "signal_files = [f for f in data_files if not os.path.basename(f).lower().startswith('background')]\n",
    "\n",
    "# Process BACKGROUND data\n",
    "if bkg_files:\n",
    "    # Assuming there's only one background file, take the first one\n",
    "    bkg_file = bkg_files[0]\n",
    "    \n",
    "    with h5py.File(bkg_file, 'r') as file:\n",
    "        full_data = file['Particles'][:,:,:-1]\n",
    "        np.random.shuffle(full_data)\n",
    "        if events: full_data = full_data[:events,:,:]\n",
    "    \n",
    "    # Split off 10% for discovery testing\n",
    "    discovery_data, remaining_data = train_test_split(full_data, test_size=0.9, shuffle=True)\n",
    "    \n",
    "    # define training, test and validation datasets from the remaining 90%\n",
    "    X_train, X_test = train_test_split(remaining_data, test_size=test_size, shuffle=True)\n",
    "    X_train, X_val = train_test_split(X_train, test_size=val_size)\n",
    "    \n",
    "    del full_data\n",
    "    del remaining_data\n",
    "    \n",
    "    # flatten the data for model input\n",
    "    X_train = X_train.reshape(X_train.shape[0], input_shape)\n",
    "    X_test = X_test.reshape(X_test.shape[0], input_shape)\n",
    "    X_val = X_val.reshape(X_val.shape[0], input_shape)\n",
    "    discovery_data = discovery_data.reshape(discovery_data.shape[0], input_shape)\n",
    "    \n",
    "    # Save background dataset\n",
    "    bkg_basename = os.path.splitext(os.path.basename(bkg_file))[0]\n",
    "    with h5py.File(f'../data/processed_data/{bkg_basename}_dataset.h5', 'w') as h5f:\n",
    "        h5f.create_dataset('X_train', data = X_train)\n",
    "        h5f.create_dataset('X_test', data = X_test)\n",
    "        h5f.create_dataset('X_val', data = X_val)\n",
    "        h5f.create_dataset('X_discovery', data = discovery_data)\n",
    "\n",
    "# Process SIGNAL files\n",
    "for signal_file in signal_files:\n",
    "    with h5py.File(signal_file, 'r') as f:\n",
    "        signal_data = f['Particles'][:,:,:-1]\n",
    "        signal_data = signal_data.reshape(signal_data.shape[0], input_shape)\n",
    "        \n",
    "        # Save signal dataset\n",
    "        signal_basename = os.path.splitext(os.path.basename(signal_file))[0]\n",
    "        with h5py.File(f'../data/processed_data/{signal_basename}_dataset.h5', 'w') as h5f2:\n",
    "            h5f2.create_dataset('Data', data = signal_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(ML_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
